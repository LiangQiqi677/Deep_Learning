{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import jieba\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/liangqiqi/anaconda3/envs/pytorch_env/lib/python3.7/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmpy_bd8p25' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.211 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "sample_list=[]\n",
    "with open('virus_train.txt','r', encoding = 'utf-8') as file_open:\n",
    "    data = json.load(file_open)\n",
    "    stop_word = ['【', '】', ')', '(', '、', '，', '“', '”', '。', '\\n', '《', '》', ' ', '-', '！', '？', '.', \n",
    "                 '\\'', '[', ']','：', '/', '.', '\"', ':', '’', '．', ',', '…', '?','；','（','）','@','_']\n",
    "    n = 0\n",
    "    for line in data:\n",
    "        sample_content = line['content']                                         #取出内容\n",
    "        for i in stop_word:                                                      #去掉标点等词\n",
    "            sample_content = sample_content.replace(i, \"\")\n",
    "        sentence_list=[word for word in jieba.cut(sample_content,cut_all=False)] #采用精确模式分词\n",
    "        sample_list.append(sentence_list)                                        #将分好的词加入到sample_list中\n",
    "        n += 1\n",
    "        if n > 10:\n",
    "            break\n",
    "#print(sample_list)   #[['天使'], ['致敬', '心心', '小凡', '也', '要', '做好', '防护', '措施'...]...]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(sample_list)\n",
    "sample_features = []                                                             #提取出corpus中每句话的词，不要次数\n",
    "for sentence in sample_list:\n",
    "    words = []\n",
    "    for word in sentence:\n",
    "        words.append(dictionary.token2id[word])\n",
    "    sample_features.append(words)\n",
    "voc_size = len(dictionary.token2id)                                              #获得字典大小\n",
    "#print(sample_features)  #[[0], [15, 12, 10, 2, 16, 3, 18, 14, 8, 1, 5, 4, 15, 11, 9, 6, 17, 13, 7]...]\n",
    "\n",
    "WINDOWS = 1                                                                      #取左右窗口的词作为context_word\n",
    "pairs = []                                                                       #存放训练对\n",
    "for sentence in sample_features:\n",
    "    for center_word_index in range(len(sentence)):\n",
    "        center_word_ix = sentence[center_word_index]\n",
    "        for win in range(-WINDOWS, WINDOWS+1):\n",
    "            contenx_word_index = center_word_index + win\n",
    "            if 0 <= contenx_word_index <= len(sentence)-1 and contenx_word_index != center_word_index:\n",
    "                context_word_ix = sentence[contenx_word_index]\n",
    "                pairs.append((center_word_ix, context_word_ix))\n",
    "#print('(中心词,背景词)对大小：', len(pairs))\n",
    "#print('字典大小：', voc_size)\n",
    "#print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden = torch.nn.Linear(self.vocab_size, self.embedding_size)\n",
    "        self.predict = torch.nn.Linear(self.embedding_size, self.vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.predict(x).view(1,-1)\n",
    "        out = torch.nn.functional.softmax(x,dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 4.7191\n",
      "epoch 2, loss 4.7054\n",
      "epoch 3, loss 4.6854\n",
      "epoch 4, loss 4.6364\n",
      "epoch 5, loss 4.5714\n",
      "epoch 6, loss 4.5011\n",
      "epoch 7, loss 4.4548\n",
      "epoch 8, loss 4.4209\n",
      "epoch 9, loss 4.3921\n",
      "epoch 10, loss 4.3798\n",
      "epoch 11, loss 4.3744\n",
      "epoch 12, loss 4.3678\n",
      "epoch 13, loss 4.3678\n",
      "epoch 14, loss 4.3645\n",
      "epoch 15, loss 4.3645\n",
      "epoch 16, loss 4.3645\n",
      "epoch 17, loss 4.3645\n",
      "epoch 18, loss 4.3645\n",
      "epoch 19, loss 4.3645\n",
      "epoch 20, loss 4.3613\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1'\n",
    "model = SkipGram(voc_size, 64).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "num_epoch = 20\n",
    "\n",
    "losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    batch_num, loss_sum = 0, 0.0\n",
    "    for i, (center_ix, context_ix) in enumerate(pairs):\n",
    "        data_x = torch.Tensor(np.eye(voc_size)[center_ix]).float().to(device)\n",
    "        y_pred = model(data_x)\n",
    "        y_true = torch.Tensor([context_ix]).long().to(device)\n",
    "        loss = loss_fun(y_pred.view(1, -1), y_true)\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_num += 1\n",
    "    losses.append(loss_sum/batch_num)\n",
    "    print('epoch %d, loss %.4f' % (epoch+1, losses[epoch]))\n",
    "\n",
    "# model_filename='skipgram_{}.pkl'.format(date_str)\n",
    "# torch.save(net.state_dict(),'saved_model/{}'.format(model_filename))\n",
    "# print('model is saved as {}'.format(model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
